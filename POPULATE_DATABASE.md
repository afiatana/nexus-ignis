# ğŸš€ RUN PIPELINE UNTUK POPULATE DATABASE

## âŒ Masalah yang Terjadi

Search menampilkan "TIDAK ADA HASIL" karena **database masih kosong!**

Pipeline scraping belum pernah dijalankan, jadi:
- âŒ URL mati belum divalidasi
- âŒ Snapshot dari Wayback Machine belum diambil  
- âŒ Data belum masuk database PostgreSQL
- âŒ Search engine tidak punya data untuk dicari

---

## âœ… Solusi: Jalankan Pipeline

Ada **2 cara** untuk populate database:

### **Opsi 1: Manual Run Lokal (RECOMMENDED - Cepat!)**

#### **Step 1: Setup DATABASE_URL**

Buat file `.env`:

```bash
# Copy dari Railway
# Format: postgresql://postgres:password@host:port/railway
DATABASE_URL=your_database_url_from_railway
```

**Cara dapat DATABASE_URL dari Railway:**
1. Railway Dashboard â†’ Postgres service
2. Tab "Variables" â†’ Copy `DATABASE_URL`
3. Paste ke file `.env` di atas

#### **Step 2: Install Dependencies**

```bash
cd "c:\Users\q3j5c\OneDrive\Desktop\My Search Engine"
pip install -r requirements.txt
```

#### **Step 3: Run Pipeline!**

```bash
python main.py
```

Expected output:
```
=== STARTING ARCHIVE PIPELINE ===

--- [1] COLLECTOR & VALIDATOR ---
âœ… Validating URLs...
âœ… Found X dead URLs

--- [2] ARCHIVE RETRIEVER ---
âœ… Fetching from Wayback Machine...
âœ… Retrieved X snapshots

--- [3] DB INSERTION ---
âœ… Inserting into database...
[DB] Berhasil upsert X data.

=== PIPELINE COMPLETED SUCCESSFULLY ===
```

**Timeline:** 5-10 menit tergantung jumlah URL

---

### **Opsi 2: Trigger GitHub Actions (Auto, tapi tunggu lama)**

#### **Manual Trigger GitHub Actions:**

1. Buka **GitHub repository** â†’ Tab **"Actions"**
2. Pilih workflow **"Daily Scraper & Archiver"**
3. Klik **"Run workflow"** dropdown
4. Pilih branch **"master"**
5. Klik **"Run workflow"** button
6. Tunggu ~5-10 menit

**CONDITIONS:**
- âš ï¸ Harus sudah set `DATABASE_URL` di GitHub Secrets
- âš ï¸ Workflow berjalan di Railway (butuh credits/active plan)
- âš ï¸ Lebih lambat dari run lokal

---

## ğŸ¯ RECOMMENDED: Run Lokal Sekarang

Jalankan commands ini berurutan:

### **1. Buat .env file**

```powershell
cd "c:\Users\q3j5c\OneDrive\Desktop\My Search Engine"

# Buat file .env (akan open di notepad)
notepad .env
```

Lalu isi dengan:
```
DATABASE_URL=postgresql://postgres:PASSWORD@HOST:PORT/railway
```
*Ganti dengan connection string dari Railway!*

Save & close.

### **2. Install dependencies (jika belum)**

```powershell
pip install -r requirements.txt
```

### **3. Run pipeline!**

```powershell
python main.py
```

---

## ğŸ“Š Expected Pipeline Flow

```
START
  â†“
[1] COLLECTOR
  â†’ Check kaskus.co.id (dead? âœ…)
  â†’ Check friendster.com (dead? âœ…)
  â†’ Check myspace.com (dead? âœ…)
  â†’ Write to dead_urls.txt
  â†“
[2] RETRIEVER
  â†’ Query Wayback Machine for kaskus.co.id
  â†’ Get snapshot HTML
  â†’ Extract clean text
  â†’ Save to archive_data.json
  â†“
[3] DB INDEXER
  â†’ Read archive_data.json
  â†’ INSERT into PostgreSQL
  â†’ Auto-detect category
  â†“
DONE! âœ…
  â†’ Database sekarang ada data
  â†’ Search "kaskus" = ADA HASIL! ğŸ‰
```

---

## ğŸ” Verify Database Setelah Run

### **Query di Railway:**

```sql
-- Check total data
SELECT COUNT(*) FROM archived_documents;

-- Check apakah kaskus ada
SELECT original_url, category, created_at 
FROM archived_documents 
WHERE original_url LIKE '%kaskus%';

-- Search test
SELECT original_url, 
       ts_headline('indonesian', cleaned_text, plainto_tsquery('indonesian', 'kaskus')) as snippet
FROM archived_documents
WHERE to_tsvector('indonesian', cleaned_text) @@ plainto_tsquery('indonesian', 'kaskus')
LIMIT 5;
```

---

## ğŸš¨ Troubleshooting

### **Error: "DATABASE_URL not set"**

```bash
Solusi: Buat file .env dengan DATABASE_URL dari Railway
```

### **Error: "No module named 'requests'"**

```bash
Solusi: pip install -r requirements.txt
```

### **Error: "Connection refused" / Database error**

```bash
Solusi: 
1. Cek DATABASE_URL benar
2. Cek Railway database sedang running
3. Cek internet connection
```

### **Wayback Machine tidak menemukan snapshot**

```bash
Normal! Tidak semua URL punya snapshot.
Pipeline akan skip URL tanpa snapshot.
```

---

## â±ï¸ Expected Timeline

| Step | Duration | Output |
|------|----------|--------|
| Collector | 1-2 min | Validate ~12 URLs |
| Retriever | 3-5 min | Fetch snapshots dari Wayback |
| DB Insert | 10 sec | Insert ~5-10 records |
| **TOTAL** | **5-10 min** | Database populated! |

---

## ğŸ“‹ Checklist

Sebelum run pipeline:
- [ ] âœ… Railway Postgres database running
- [ ] âœ… DATABASE_URL sudah ada (dari Railway)
- [ ] âœ… File `.env` sudah dibuat dengan DATABASE_URL
- [ ] âœ… Dependencies sudah install (`pip install -r requirements.txt`)
- [ ] âœ… seed_list.txt ada URLs (sudah ada âœ…)

Run pipeline:
- [ ] âš ï¸ `python main.py` (LAKUKAN SEKARANG!)

After pipeline:
- [ ] Verify data di database (query di atas)
- [ ] Test search di web app
- [ ] Search "kaskus" should show results! ğŸ‰

---

## ğŸŠ After Success

Setelah pipeline berhasil:

âœ… Database populated dengan archived content  
âœ… Search "kaskus" akan tampil hasil  
âœ… Full-text search works  
âœ… Category auto-detected  
âœ… GitHub Actions akan run otomatis setiap hari jam 2 pagi  

---

**Ready? Jalankan commands di atas sekarang untuk populate database! ğŸš€**

Share output jika ada error! ğŸ˜Š
